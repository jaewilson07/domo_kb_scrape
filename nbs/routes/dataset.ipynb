{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Routes\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: dataset_routes.html\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp routes.dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "from typing import Optional\n",
    "\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "import aiohttp\n",
    "\n",
    "import domolibrary.client.get_data as gd\n",
    "import domolibrary.client.ResponseGetData as rgd\n",
    "import domolibrary.client.DomoAuth as dmda\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DatasetNotFoundError(Exception):\n",
    "    def __init__(self, dataset_id, domo_instance):\n",
    "        message = f\"dataset - {dataset_id} not found in {domo_instance}\"\n",
    "\n",
    "        super().__init__(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class QueryRequestError(Exception):\n",
    "    def __init__(self, dataset_id, domo_instance, sql):\n",
    "        message = f\"dataset - {dataset_id} in {domo_instance} received a bad request.  Check your SQL \\n {sql}\"\n",
    "\n",
    "        super().__init__(message)\n",
    "\n",
    "# typically do not use\n",
    "async def query_dataset_public(\n",
    "    dev_auth: dmda.DomoDeveloperAuth,\n",
    "    dataset_id: str,\n",
    "    sql: str,\n",
    "    session: aiohttp.ClientSession,\n",
    "    debug_api: bool = False,\n",
    "):\n",
    "\n",
    "    \"\"\"query for hitting public apis, requires client_id and secret authentication\"\"\"\n",
    "\n",
    "    url = f\"https://api.domo.com/v1/datasets/query/execute/{dataset_id}?IncludeHeaders=true\"\n",
    "\n",
    "    body = {\"sql\": sql}\n",
    "\n",
    "    return await gd.get_data(\n",
    "        auth=dev_auth, url=url, method=\"POST\", body=body, session=session, debug_api=debug_api)\n",
    "        \n",
    "\n",
    "        \n",
    "async def query_dataset_private(\n",
    "    auth: dmda.DomoAuth,  # DomoFullAuth or DomoTokenAuth\n",
    "    dataset_id: str,\n",
    "    sql: str,\n",
    "    session: Optional[aiohttp.ClientSession] = None,\n",
    "    loop_until_end: bool = False,  # retrieve all available rows\n",
    "    limit=100,  # maximum rows to return per request.  refers to PAGINATION\n",
    "    skip=0,\n",
    "    maximum=100,  # equivalent to the LIMIT or TOP clause in SQL, the number of rows to return total\n",
    "    debug_api: bool = False,\n",
    "    debug_loop: bool = False,\n",
    "):\n",
    "    \"\"\"execute SQL queries against private APIs, requires DomoFullAuth or DomoTokenAuth\"\"\"\n",
    "\n",
    "    url = f\"https://{auth.domo_instance}.domo.com/api/query/v1/execute/{dataset_id}\"\n",
    "\n",
    "    offset_params = {\n",
    "        \"offset\": \"offset\",\n",
    "        \"limit\": \"limit\",\n",
    "    }\n",
    "\n",
    "    def body_fn(skip, limit):\n",
    "        return {\"sql\": f\"{sql} limit {limit} offset {skip}\"}\n",
    "\n",
    "    def arr_fn(res) -> pd.DataFrame:\n",
    "        rows_ls = res.response.get(\"rows\")\n",
    "        columns_ls = res.response.get(\"columns\")\n",
    "        output = []\n",
    "        for row in rows_ls:\n",
    "            new_row = {}\n",
    "            for index, column in enumerate(columns_ls):\n",
    "                new_row[column] = row[index]\n",
    "            output.append(new_row)\n",
    "            # pd.DataFrame(data=res.response.get('rows'), columns=res.response.get('columns'))\n",
    "        return output\n",
    "\n",
    "    res = await gd.looper(\n",
    "        auth=auth,\n",
    "        method=\"POST\",\n",
    "        url=url,\n",
    "        arr_fn=arr_fn,\n",
    "        offset_params=offset_params,\n",
    "        limit=limit,\n",
    "        skip=skip,\n",
    "        maximum=maximum,\n",
    "        session=session,\n",
    "        body_fn=body_fn,\n",
    "        debug_api=debug_api,\n",
    "        debug_loop=debug_loop,\n",
    "        loop_until_end=loop_until_end\n",
    "    )\n",
    "\n",
    "    if res.status == 404 and res.response == 'Not Found':\n",
    "        raise DatasetNotFoundError(dataset_id=dataset_id , domo_instance=auth.domo_instance)\n",
    "    \n",
    "    if res.status == 400 and res.response == 'Bad Request':\n",
    "        raise QueryRequestError(dataset_id=dataset_id , domo_instance=auth.domo_instance, sql = sql)\n",
    "    \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>objectID</th>\n",
       "      <th>url</th>\n",
       "      <th>Title</th>\n",
       "      <th>article</th>\n",
       "      <th>views</th>\n",
       "      <th>created_dt</th>\n",
       "      <th>published_dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000004790</td>\n",
       "      <td>https://domo-support.domo.com/s/article/360046...</td>\n",
       "      <td>Starting, Stopping, and Restarting the Workben...</td>\n",
       "      <td>Important:  Support for Workbench 4 ended on ...</td>\n",
       "      <td>39</td>\n",
       "      <td>2022-10-24T22:30:00</td>\n",
       "      <td>2022-10-24T22:41:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000004796</td>\n",
       "      <td>https://domo-support.domo.com/s/article/360047...</td>\n",
       "      <td>Understanding the Workbench 4 User Interface</td>\n",
       "      <td>Important:  Support for Workbench 4 ended on ...</td>\n",
       "      <td>56</td>\n",
       "      <td>2022-10-24T22:30:00</td>\n",
       "      <td>2022-10-24T22:40:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000004773</td>\n",
       "      <td>https://domo-support.domo.com/s/article/360046...</td>\n",
       "      <td>Using the External Process File Provider in Wo...</td>\n",
       "      <td>Important:  Support for Workbench 4 ended on ...</td>\n",
       "      <td>20</td>\n",
       "      <td>2022-10-24T22:30:00</td>\n",
       "      <td>2022-10-24T22:41:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000004798</td>\n",
       "      <td>https://domo-support.domo.com/s/article/360046...</td>\n",
       "      <td>Workbench 4 FAQs</td>\n",
       "      <td>Important:  Support for Workbench 4 ended on ...</td>\n",
       "      <td>48</td>\n",
       "      <td>2022-10-24T22:30:00</td>\n",
       "      <td>2022-10-24T22:40:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000004800</td>\n",
       "      <td>https://domo-support.domo.com/s/article/360047...</td>\n",
       "      <td>Workbench 4 Overview</td>\n",
       "      <td>Important:  Support for Workbench 4 ended on ...</td>\n",
       "      <td>40</td>\n",
       "      <td>2022-10-24T22:30:00</td>\n",
       "      <td>2022-10-24T22:41:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    objectID                                                url  \\\n",
       "0  000004790  https://domo-support.domo.com/s/article/360046...   \n",
       "1  000004796  https://domo-support.domo.com/s/article/360047...   \n",
       "2  000004773  https://domo-support.domo.com/s/article/360046...   \n",
       "3  000004798  https://domo-support.domo.com/s/article/360046...   \n",
       "4  000004800  https://domo-support.domo.com/s/article/360047...   \n",
       "\n",
       "                                               Title  \\\n",
       "0  Starting, Stopping, and Restarting the Workben...   \n",
       "1       Understanding the Workbench 4 User Interface   \n",
       "2  Using the External Process File Provider in Wo...   \n",
       "3                                   Workbench 4 FAQs   \n",
       "4                               Workbench 4 Overview   \n",
       "\n",
       "                                             article  views  \\\n",
       "0   Important:  Support for Workbench 4 ended on ...     39   \n",
       "1   Important:  Support for Workbench 4 ended on ...     56   \n",
       "2   Important:  Support for Workbench 4 ended on ...     20   \n",
       "3   Important:  Support for Workbench 4 ended on ...     48   \n",
       "4   Important:  Support for Workbench 4 ended on ...     40   \n",
       "\n",
       "            created_dt         published_dt  \n",
       "0  2022-10-24T22:30:00  2022-10-24T22:41:00  \n",
       "1  2022-10-24T22:30:00  2022-10-24T22:40:00  \n",
       "2  2022-10-24T22:30:00  2022-10-24T22:41:00  \n",
       "3  2022-10-24T22:30:00  2022-10-24T22:40:00  \n",
       "4  2022-10-24T22:30:00  2022-10-24T22:41:00  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-dojo\",\n",
    "    domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "sql = f\"SELECT * FROM TABLE\"\n",
    "\n",
    "ds_res = await query_dataset_private(dataset_id=os.environ['DOJO_DATASET_ID'],\n",
    "                                     auth=token_auth,\n",
    "                                     sql=sql,\n",
    "                                     skip=42,\n",
    "                                     maximum=5,\n",
    "                                     loop_until_end=False)\n",
    "pd.DataFrame(ds_res.response)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "async def get_dataset_by_id(\n",
    "    dataset_id: str, # dataset id from URL\n",
    "    auth: Optional[dmda.DomoAuth] = None, # requires full authentication\n",
    "    debug_api: bool = False, # for troubleshooting API request\n",
    "    session: Optional[aiohttp.ClientSession] = None\n",
    ") -> rgd.ResponseGetData: # returns metadata about a dataset\n",
    "    \"\"\"retrieve dataset metadata\"\"\"\n",
    "\n",
    "    url = f\"https://{auth.domo_instance}.domo.com/api/data/v3/datasources/{dataset_id}\"\n",
    "\n",
    "    res= await gd.get_data(\n",
    "        auth=auth,\n",
    "        url=url,\n",
    "        method=\"GET\",\n",
    "        debug_api=debug_api, session = session\n",
    "    )\n",
    "\n",
    "    if res.status == 404 and res.response == 'Not Found':\n",
    "        raise DatasetNotFoundError(dataset_id=dataset_id, domo_instance=auth.domo_instance)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    token_auth = dmda.DomoTokenAuth(\n",
    "        domo_instance=\"domo-dojo\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    "    )\n",
    "\n",
    "    await get_dataset_by_id(dataset_id=123, auth=token_auth)\n",
    "\n",
    "except DatasetNotFoundError as e:\n",
    "    print(e) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-dojo\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "ds_res = await get_dataset_by_id(dataset_id=os.environ['DOJO_DATASET_ID'], auth=token_auth)\n",
    "pd.DataFrame([ds_res.response])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "async def get_schema(\n",
    "    auth: dmda.DomoAuth, dataset_id: str, debug_api: bool = False\n",
    ") -> rgd.ResponseGetData:\n",
    "    \"\"\"retrieve the schema for a dataset\"\"\"\n",
    "\n",
    "    url = f\"https://{auth.domo_instance}.domo.com/api/query/v1/datasources/{dataset_id}/schema/indexed?includeHidden=false\"\n",
    "\n",
    "    return await gd.get_data(auth=auth, url=url, method=\"GET\", debug_api=debug_api)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample implementation of get_schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-dojo\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "ds_res = await get_schema(dataset_id=os.environ['DOJO_DATASET_ID'], auth=token_auth)\n",
    "pd.DataFrame(ds_res.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve schema from response\n",
    "pd.DataFrame(ds_res.response.get(\"tables\")[0].get(\"columns\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def set_dataset_tags(auth: dmda.DomoFullAuth,\n",
    "                           tag_ls: [str], # complete list of tags for dataset\n",
    "                           dataset_id: str,\n",
    "                           debug_api: bool = False,\n",
    "                           session: Optional[aiohttp.ClientSession] = None,\n",
    "                           ):\n",
    "    \n",
    "    \"\"\"REPLACE tags on this dataset with a new list\"\"\"\n",
    "\n",
    "    url = f\"https://{auth.domo_instance}.domo.com/api/data/ui/v3/datasources/{dataset_id}/tags\"\n",
    "\n",
    "    res = await gd.get_data(\n",
    "        auth=auth,\n",
    "        url=url,\n",
    "        method='POST',\n",
    "        debug_api=debug_api,\n",
    "        body=tag_ls,\n",
    "        session=session\n",
    "    )\n",
    "\n",
    "    if res.status == 200:\n",
    "        res.set_response (response = f'Dataset {dataset_id} tags updated to [{ \", \".join(tag_ls) }]')\n",
    "    \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"],\n",
    "    domo_instance=\"domo-dojo\",\n",
    ")\n",
    "\n",
    "tag_ls = ['hackercore', 'developer_documentation']\n",
    "\n",
    "await set_dataset_tags(auth=token_auth,\n",
    "                       tag_ls=tag_ls,\n",
    "                       dataset_id=os.environ['DOJO_DATASET_ID'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Data\n",
    "\n",
    "#### overview\n",
    "\n",
    "In the URL, parts refers to the multi-part API and is unrelated to the partitions concept. The multi-part API was designed to allow sending multiple streams of Data into a data_version simultaneously.\n",
    "\n",
    "In stage 1, the values passed in the Body will be superseded by values in the COMMIT (stage 3), so best practices is to not populate values here.\n",
    "\n",
    "The response includes an uploadId, which must be stored and passed to the URL of the subsequent upload request (stages 2 and 3).\n",
    "\n",
    "#### url params\n",
    "\n",
    "The dataTag parameter allows users to UPDATE or REPLACE a datatag (partition)\n",
    "\n",
    "NOTE: restateDataTag is largely deprecated // exists for backward compatibility\n",
    "\n",
    "#### body params\n",
    "\n",
    "The appendId parameter accepts \"latest\" or \"None\"\n",
    "\n",
    "latest will APPEND the data version to the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class UploadDataError(Exception):\n",
    "    \"\"\"raise if unable to upload data to Domo\"\"\"\n",
    "    \n",
    "    def __init__(self, stage_num : int, dataset_id : str, domo_instance : str):\n",
    "        message = f\"error uploading data to {dataset_id} during Stage { stage_num} in {domo_instance}\"\n",
    "        super().__init__(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "async def upload_dataset_stage_1(auth: dmda.DomoAuth,\n",
    "                                 dataset_id: str,\n",
    "                                 #  restate_data_tag: str = None, # deprecated\n",
    "                                 partition_tag: str = None,  # synonymous with data_tag\n",
    "                                 session: Optional[aiohttp.ClientSession] = None,\n",
    "                                 debug_api: bool = False,\n",
    "                                 ) -> rgd.ResponseGetData:\n",
    "\n",
    "    \"\"\"preps dataset for upload by creating an upload_id (upload session key) pass to stage 2 as a parameter\"\"\"\n",
    "\n",
    "    url = f\"https://{auth.domo_instance}.domo.com/api/data/v3/datasources/{dataset_id}/uploads\"\n",
    "\n",
    "    # base body assumes no paritioning\n",
    "    body = {\n",
    "        \"action\": None,\n",
    "        \"appendId\": None\n",
    "    }\n",
    "\n",
    "    params = None\n",
    "\n",
    "    if partition_tag:\n",
    "        # params = {'dataTag': restate_data_tag or data_tag} # deprecated\n",
    "        params = {'dataTag': partition_tag}\n",
    "        body.update({'appendId': 'latest'})\n",
    "\n",
    "    res = await gd.get_data(auth=auth,\n",
    "                         url=url, method='POST',\n",
    "                         body=body,\n",
    "                         session=session,\n",
    "                         debug_api=debug_api,\n",
    "                         params=params)\n",
    "\n",
    "    if not res.is_success:\n",
    "        raise UploadDataError(\n",
    "            stage_num=1, dataset_id=dataset_id, domo_instance=auth.domo_instance)\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "async def upload_dataset_stage_2_file(\n",
    "    auth: dmda.DomoAuth,\n",
    "    dataset_id: str,\n",
    "    upload_id: str,  # must originate from  a stage_1 upload response\n",
    "    data_file: Optional[io.TextIOWrapper] = None,\n",
    "    session: Optional[aiohttp.ClientSession] = None,\n",
    "    # only necessary if streaming multiple files into the same partition (multi-part upload)\n",
    "    part_id: str = 2,\n",
    "    debug_api: bool = False,\n",
    ") -> rgd.ResponseGetData:\n",
    "\n",
    "    url = f\"https://{auth.domo_instance}.domo.com/api/data/v3/datasources/{dataset_id}/uploads/{upload_id}/parts/{part_id}\"\n",
    "\n",
    "    body = data_file\n",
    "\n",
    "    res = await gd.get_data(\n",
    "        url=url,\n",
    "        method=\"PUT\",\n",
    "        auth=auth,\n",
    "        content_type=\"text/csv\",\n",
    "        body=body,\n",
    "        session=session,\n",
    "        debug_api=debug_api,\n",
    "    )\n",
    "    if not res.is_success:\n",
    "        raise UploadDataError(stage_num = 2 , dataset_id = dataset_id, domo_instance = auth.domo_instance)\n",
    "\n",
    "    res.upload_id = upload_id\n",
    "    res.dataset_id = dataset_id\n",
    "    res.part_id = part_id\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "async def upload_dataset_stage_2_df(\n",
    "    auth: dmda.DomoAuth,\n",
    "    dataset_id: str,\n",
    "    upload_id: str,  # must originate from  a stage_1 upload response\n",
    "    upload_df: pd.DataFrame,\n",
    "    session: Optional[aiohttp.ClientSession] = None,\n",
    "    part_id: str = 2,  # only necessary if streaming multiple files into the same partition (multi-part upload)\n",
    "    debug_api: bool = False,\n",
    ") -> rgd.ResponseGetData:\n",
    "\n",
    "    url = f\"https://{auth.domo_instance}.domo.com/api/data/v3/datasources/{dataset_id}/uploads/{upload_id}/parts/{part_id}\"\n",
    "\n",
    "    body = upload_df.to_csv(header=False, index=False)\n",
    "\n",
    "    # if debug:\n",
    "    #     print(body)\n",
    "\n",
    "    res = await gd.get_data(\n",
    "        url=url,\n",
    "        method=\"PUT\",\n",
    "        auth=auth,\n",
    "        content_type=\"text/csv\",\n",
    "        body=body,\n",
    "        session=session,\n",
    "        debug_api=debug_api,\n",
    "    )\n",
    "\n",
    "    if not res.is_success:\n",
    "        raise UploadDataError(stage_num = 2 , dataset_id = dataset_id, domo_instance = auth.domo_instance)\n",
    "\n",
    "    res.upload_id = upload_id\n",
    "    res.dataset_id = dataset_id\n",
    "    res.part_id = part_id\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "async def upload_dataset_stage_3(\n",
    "    auth: dmda.DomoAuth,\n",
    "    dataset_id: str,\n",
    "    upload_id: str,  # must originate from  a stage_1 upload response\n",
    "    session: Optional[aiohttp.ClientSession] = None,\n",
    "    update_method: str = \"REPLACE\",  # accepts REPLACE or APPEND\n",
    "    #  restate_data_tag: str = None, # deprecated\n",
    "    partition_tag: str = None,  # synonymous with data_tag\n",
    "    is_index: bool = False,  # index after uploading\n",
    "    debug_api: bool = False,\n",
    ") -> rgd.ResponseGetData:\n",
    "\n",
    "    \"\"\"commit will close the upload session, upload_id.  this request defines how the data will be loaded into Adrenaline, update_method\n",
    "    has optional flag for indexing dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    url = f\"https://{auth.domo_instance}.domo.com/api/data/v3/datasources/{dataset_id}/uploads/{upload_id}/commit\"\n",
    "\n",
    "    body = {\"index\": is_index, \"action\": update_method}\n",
    "\n",
    "    if partition_tag:\n",
    "\n",
    "        body.update(\n",
    "            {\n",
    "                \"action\": \"APPEND\",\n",
    "                #  'dataTag': restate_data_tag or data_tag,\n",
    "                #  'appendId': 'latest' if (restate_data_tag or data_tag) else None,\n",
    "                \"dataTag\": partition_tag,\n",
    "                \"appendId\": \"latest\" if partition_tag else None,\n",
    "                \"index\": is_index,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    res = await gd.get_data(\n",
    "        auth=auth, method=\"PUT\", url=url, body=body, session=session, debug_api=debug_api\n",
    "    )\n",
    "\n",
    "    if not res.is_success:\n",
    "        raise UploadDataError(stage_num = 3 , dataset_id = dataset_id, domo_instance = auth.domo_instance)\n",
    "\n",
    "    res.upload_id = upload_id\n",
    "    res.dataset_id = dataset_id\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "async def index_dataset(\n",
    "    auth: dmda.DomoAuth,\n",
    "    dataset_id: str,\n",
    "    session: Optional[aiohttp.ClientSession] = None,\n",
    "    debug_api: bool = False,\n",
    ") -> rgd.ResponseGetData:\n",
    "    \"\"\"manually index a dataset\"\"\"\n",
    "\n",
    "    url = f\"https://{auth.domo_instance}.domo.com/api/data/v3/datasources/{dataset_id}/indexes\"\n",
    "\n",
    "    body = {\"dataIds\": []}\n",
    "\n",
    "    return await gd.get_data(\n",
    "        auth=auth, method=\"POST\", body=body, url=url, session=session, debug_api = debug_api\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "async def index_status(\n",
    "    auth: dmda.DomoAuth,\n",
    "    dataset_id: str,\n",
    "    index_id: str,\n",
    "    session: Optional[aiohttp.ClientSession] = None,\n",
    "    debug_api: bool = False,\n",
    ") -> rgd.ResponseGetData:\n",
    "    \"\"\"get the completion status of an index\"\"\"\n",
    "\n",
    "    url = f\"https://{auth.domo_instance}.domo.com/api/data/v3/datasources/{dataset_id}/indexes/{index_id}/statuses\"\n",
    "\n",
    "    return await gd.get_data(\n",
    "        auth=auth, \n",
    "        method=\"GET\", url=url, \n",
    "        session=session, debug_api=debug_api\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# def generate_list_partitions_body(limit=100, offset=0):\n",
    "#     return {\n",
    "#         \"paginationFields\": [{\n",
    "#             \"fieldName\": \"datecompleted\",\n",
    "#             \"sortOrder\": \"DESC\",\n",
    "#             \"filterValues\": {\n",
    "#                 \"MIN\": None,\n",
    "#                 \"MAX\": None\n",
    "#             }\n",
    "#         }],\n",
    "#         \"limit\": 1000,\n",
    "#         \"offset\": 0\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# async def list_partitions(full_auth: DomoFullAuth,\n",
    "#                           dataset_id: str,\n",
    "#                           body: dict = None,\n",
    "#                           maximum: int = None,\n",
    "#                           loop_until_end: bool = True,\n",
    "#                           session: aiohttp.ClientSession = None,\n",
    "#                           debug: bool = False):\n",
    "#     try:\n",
    "\n",
    "#         is_close_session = False if session else True\n",
    "\n",
    "#         if not session:\n",
    "#             session = aiohttp.ClientSession()\n",
    "\n",
    "#         body = body or generate_list_partitions_body()\n",
    "\n",
    "#         url = f\"https://{full_auth.domo_instance}.domo.com/api/query/v1/datasources/{dataset_id}/partition/list\"\n",
    "\n",
    "#         offset_params = {\n",
    "#             'offset': 'offset',\n",
    "#             'limit': 'limit',\n",
    "#         }\n",
    "\n",
    "#         def arr_fn(res) -> list[dict]:\n",
    "#             return res.response\n",
    "\n",
    "#         res = await looper(auth=full_auth,\n",
    "#                            method='POST',\n",
    "#                            url=url,\n",
    "#                            arr_fn=arr_fn,\n",
    "#                            body=body,\n",
    "#                            offset_params_in_body=True,\n",
    "#                            offset_params=offset_params,\n",
    "#                            loop_until_end=True,\n",
    "#                            session=session,\n",
    "#                            debug=debug)\n",
    "\n",
    "#         if isinstance(res, list):\n",
    "#             return ResponseGetData(status=200,\n",
    "#                                    response=res,\n",
    "#                                    is_success=True)\n",
    "#         else:\n",
    "#             return ResponseGetData(status=400,\n",
    "#                                    response=None,\n",
    "#                                    is_success=False)\n",
    "\n",
    "#     finally:\n",
    "#         if is_close_session:\n",
    "#             await session.close()\n",
    "\n",
    "# # Delete partition has 3 stages\n",
    "# # Stage 1. This marks the data version associated with the partition tag as deleted.  It does not delete the partition tag or remove the association between the partition tag and data version.  There should be no need to upload an empty file – step #3 will remove the data from Adrenaline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# async def delete_partition_stage_1(full_auth: DomoFullAuth,\n",
    "#                                    dataset_id: str,\n",
    "#                                    dataset_partition_id: str,\n",
    "#                                    session: aiohttp.ClientSession = None,\n",
    "#                                    debug: bool = False):\n",
    "\n",
    "#     #url = f'https://{full_auth.domo_instance}.domo.com/api/query/v1/datasources/{dataset_id}/partition/{dataset_partition_id}'\n",
    "#     # update on 9/9/2022 based on the conversation with Greg Swensen\n",
    "#     url = f'https://{full_auth.domo_instance}.domo.com/api/query/v1/datasources/{dataset_id}/tag/{dataset_partition_id}/data'\n",
    "\n",
    "#     return await get_data(\n",
    "#         auth=full_auth,\n",
    "#         method=\"DELETE\",\n",
    "#         url=url,\n",
    "#         session=session,\n",
    "#         debug=debug\n",
    "#     )\n",
    "# # Stage 2. This will remove the partition association so that it doesn’t show up in the list call.  Technically, this is not required as a partition against a deleted data version will not count against the 400 partition limit, but as the current partitions api doesn’t make that clear, cleaning these up will make it much easier for you to manage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# async def delete_partition_stage_2(full_auth: DomoFullAuth,\n",
    "#                                    dataset_id: str,\n",
    "#                                    dataset_partition_id: str,\n",
    "#                                    session: aiohttp.ClientSession = None,\n",
    "#                                    debug: bool = False):\n",
    "\n",
    "#     url = f'https://{full_auth.domo_instance}.domo.com/api/query/v1/datasources/{dataset_id}/partition/{dataset_partition_id}'\n",
    "\n",
    "#     return await get_data(\n",
    "#         auth=full_auth,\n",
    "#         method=\"DELETE\",\n",
    "#         url=url,\n",
    "#         session=session,\n",
    "#         debug=debug\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# async def delete(full_auth: DomoFullAuth,\n",
    "#                  dataset_id: str, session: aiohttp.ClientSession = None, debug: bool = False):\n",
    "#     url = f\"https://{full_auth.domo_instance}.domo.com/api/data/v3/datasources/{dataset_id}?deleteMethod=hard\"\n",
    "\n",
    "#     return await get_data(\n",
    "#         auth=full_auth,\n",
    "#         method=\"DELETE\",\n",
    "#         url=url,\n",
    "#         session=session,\n",
    "#         debug=debug\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
